# Jailbreaking
Jailbreaking in prompt engineering refers to techniques used to bypass an AI's ethical safeguards. It often involves carefully crafted prompts or [roleplay scenarios](https://www.wired.com/story/chatgpt-jailbreak-generative-ai-hacking/). While controversial, it raises important questions about [AI safety](https://www.anthropic.com/index/a-constitutional-ai-approach-to-ai-safety) and the robustness of language models.

- [@article@Jailbreaking - LearnPrompting](https://learnprompting.org/docs/prompt_hacking/jailbreaking)
- [@article@Jailbreaking - PromptDev](https://promptdev.ai/docs/prompt_hacking/jailbreaking)
- [@article@Jailbreaking - Research Paper](https://arxiv.org/abs/2301.12867)
